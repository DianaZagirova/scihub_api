# Examples Directory

This directory contains sample files demonstrating the structure and format of data generated by the paper collection pipeline.

## Contents

### ğŸ“„ `parsed_jsons/`
Example JSON files generated by the parsing pipeline:

- **`example_fast_parser.json`** - Output from PyMuPDF fast parser
  - Simpler structure, faster processing
  - Contains: title, authors, sections, full text, references
  - Processing speed: 60-180 papers/minute

- **`example_grobid_parser.json`** - Output from GROBID parser
  - More detailed structure, higher quality
  - Contains: detailed metadata, hierarchical sections, structured references, figures, tables
  - Processing speed: 15-40 papers/minute

### ğŸ“š `pdfs/`
Information about PDF storage:

- **`README.md`** - Explains PDF naming conventions, storage structure, and validation
- PDFs themselves are NOT committed (see main README)
- Typical size: 1-5 MB per paper

### ğŸ—„ï¸ `tracker/`
Processing tracker database documentation and examples:

- **`README.md`** - Complete explanation of how the tracker works, including:
  - Overview and purpose
  - Workflow integration
  - Multi-source tracking (6 download sources)
  - Database schema and status values
  - Usage examples
  
- **`example_tracker_records.json`** - Sample tracker data showing 5 different scenarios:
  1. Successful Sci-Hub download + fast parsing
  2. Open Access download + dual parser (fast + GROBID)
  3. Failed download (all sources exhausted)
  4. Downloaded but parsing failed (corrupted PDF)
  5. In progress (downloaded, parsing pending)

- **`create_example_tracker.sql`** - SQL schema and sample data for reference

The tracker is the **single source of truth** for monitoring the entire pipeline, tracking download attempts from 6 sources (Sci-Hub, Unpaywall, arXiv, bioRxiv, Europe PMC, Semantic Scholar) and parsing status from both parsers.

## Why These Files Are Not Committed

The actual data files (PDFs, parsed JSONs, tracker database) are **NOT committed to git** because:

1. **Size**: Hundreds of GB for large collections
2. **Binary Files**: PDFs are binary and bloat git history
3. **Copyright**: Papers may be copyrighted material
4. **Reproducibility**: Can be regenerated using the pipeline
5. **User-Specific**: Paths and configurations vary by user

## Generating Your Own Data

Follow the [Production Pipeline](../README.md#production-pipeline-recommended-workflow) to generate these files:

```bash
# Step 1: Generate DOI list
python src/helper_scripts/create_missing_eval.py

# Step 2: Download and parse
python download_papers_optimized.py -f missing_dois/dois_to_process.txt --parser fast -w 4

# Step 3: Sync filesystem
python status_sync/sync_processing_state_to_db.py --seed-missing

# Step 4: Update database
python update_database.py --update-from-jsons --db /path/to/papers.db --output-dir ./output
```

## File Locations in Production

When running the pipeline, files are generated in these locations:

```
scihub_api/
â”œâ”€â”€ papers/                          # Downloaded PDFs (gitignored)
â”‚   â”œâ”€â”€ 10.1016_j.cell.2023.01.001.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ output/                          # Parsed JSON files (gitignored)
â”‚   â”œâ”€â”€ 10.1016_j.cell.2023.01.001_fast.json
â”‚   â””â”€â”€ ...
â””â”€â”€ /path/to/papers.db              # Stage 1 database with tracker table
    â””â”€â”€ processing_tracker table     # Tracking data
```

## Storage Requirements

For reference, typical storage needs:

| Papers | PDFs | JSONs | Tracker | Total |
|--------|------|-------|---------|-------|
| 1,000 | 2-5 GB | 50-500 MB | <1 MB | ~3-6 GB |
| 10,000 | 20-50 GB | 500 MB-5 GB | 1-5 MB | ~25-55 GB |
| 100,000 | 200-500 GB | 5-50 GB | 5-10 MB | ~250-550 GB |

## Questions?

See the main [README.md](../README.md) for complete documentation.
